{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"napari-lattice","text":"<p>This napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Check the Wiki page for documentation on how to get started.</p> <p> </p> <p>Functions</p> <ul> <li>Deskewing and deconvolution of Zeiss lattice lightsheet images</li> <li>Ability to preview deskewed image at channel or timepoint of interest</li> <li>Crop and process only a small portion of the image </li> <li>Import ImageJ ROIs for cropping</li> <li>Create image processing workflows using napari-workflows</li> <li>Run deskewing, deconvolution and custom image processing workflows from the terminal</li> <li>Files can be saved as h5 (BigDataViewer/BigStitcher) or tiff files</li> <li>Run in terminal without napari, enabling processing workflows on the HPC</li> </ul> <p>Key Features</p> <p>Apply custom image processing workflows using <code>napari-workflows</code>.  * Interactive workflow generation (no coding experience needed) * Use custom python functions/modules within workflows * How to use Cellpose for cell segmentation</p> <p>Support will be added for more file formats in the future.</p> <p>Sample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784</p> <p>This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request.</p>"},{"location":"#license","title":"License","text":"<p>Distributed under the terms of the [GPL-3.0 License] license, \"napari_lattice\" is free and open source software</p>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was supported by funding from the Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research. This project has been made possible in part by Napari plugin accelerator grant from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.</p> <p>Thanks to the developers and maintainers of the amazing open-source plugins such as pyclesperanto, aicsimageio, dask and pycudadecon.  Thanks in particular to the developers of open source projects: LLSpy and lls_dd as they were referred to extensively for developing napari-lattice.  Thanks to the imagesc forum!</p>"},{"location":"#issues","title":"Issues","text":"<p>If you encounter any problems, please file an issue along with a detailed description.</p>"},{"location":"api/","title":"Python Usage","text":""},{"location":"api/#lls_core.LatticeData","title":"lls_core.LatticeData","text":"<p>Parameters for the entire deskewing process, including outputs and optional steps such as deconvolution. This is the recommended entry point for Python users: construct an instance of this class, and then perform the processing using methods.</p> <p>Note that none of this class's methods have any parameters: all parameters are class fields for validation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>DataArray</code> <p>A 3-5D array containing the image data. Can be anything convertible to an Xarray, including a <code>dask.array</code> or <code>numpy.ndarray</code>. Can also be provided as a <code>str</code>, in which case it must indicate the path to an image to load from disk.</p> <code>None</code> <code>skew</code> <code>DeskewDirection</code> <p>Axis along which to deskew the image. Choices: <code>{\"X\", \"Y\"}</code>. These can be provided as <code>str</code>.</p> <code>&lt;DeskewDirection.Y: 2&gt;</code> <code>angle</code> <code>float</code> <p>Angle of deskewing, in degrees, as a float.</p> <code>30.0</code> <code>physical_pixel_sizes</code> <code>DefinedPixelSizes</code> <p>Pixel size of the microscope, in microns. This can alternatively be provided as a <code>tuple[float]</code> of <code>(Z, Y, X)</code></p> <code>None</code> <code>derived</code> <code>DerivedDeskewFields</code> <p>Refer to the <code>DerivedDeskewFields</code> docstring</p> <code>None</code> <code>save_dir</code> <code>DirectoryPath</code> <p>The directory where the output data will be saved. This can be specified as a <code>str</code> or <code>Path</code>.</p> <code>None</code> <code>save_suffix</code> <code>str</code> <p>The filename suffix that will be used for output files. This will be added as a suffix to the input file name if the input image was specified using a file name. If the input image was provided as an in-memory object, the <code>save_name</code> field should instead be specified.</p> <code>'_deskewed'</code> <code>save_name</code> <code>str</code> <p>The filename that will be used for output files. This should not contain a leading directory or file extension. The final output files will have additional elements added to the end of this prefix to indicate the region of interest, channel, timepoint, file extension etc.</p> <code>None</code> <code>save_type</code> <code>SaveFileType</code> <p>The data type to save the result as. This will also be used to determine the file extension of the output files. Choices: <code>{\"h5\", \"tiff\"}</code>. Choices can alternatively be specifed as <code>str</code>, for example <code>'tiff'</code>.</p> <code>&lt;SaveFileType.h5: 'h5'&gt;</code> <code>time_range</code> <code>range</code> <p>The range of times to process. This defaults to all time points in the image array.</p> <code>None</code> <code>channel_range</code> <code>range</code> <p>The range of channels to process. This defaults to all time points in the image array.</p> <code>None</code> <code>deconvolution</code> <code>DeconvolutionParams | None</code> <p>Parameters associated with the deconvolution. If this is None, then deconvolution is disabled</p> <code>None</code> <code>crop</code> <code>CropParams | None</code> <p>Cropping parameters. If this is None, then cropping is disabled</p> <code>None</code> <code>workflow</code> <code>Workflow | None</code> <p>If defined, this is a workflow to add lightsheet processing onto</p> <code>None</code>"},{"location":"api/#lls_core.LatticeData.process","title":"process","text":"<pre><code>process() -&gt; ImageSlices\n</code></pre> <p>Execute the processing and return the result. This will not execute the attached workflow.</p>"},{"location":"api/#lls_core.LatticeData.process_workflow","title":"process_workflow","text":"<pre><code>process_workflow() -&gt; WorkflowSlices\n</code></pre> <p>Runs the workflow on each slice and returns the workflow results</p>"},{"location":"api/#lls_core.LatticeData.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Apply the processing, and saves the results to disk. Results can be found in <code>save_dir</code>.</p>"},{"location":"api/#lls_core.DeconvolutionParams","title":"lls_core.DeconvolutionParams","text":"<p>Parameters for the optional deconvolution step</p> <p>Parameters:</p> Name Type Description Default <code>decon_processing</code> <code>DeconvolutionChoice</code> <p>Hardware to use to perform the deconvolution. Choices: <code>{\"cuda_gpu\", \"opencl_gpu\", \"cpu\"}</code>. Can be provided as <code>str</code>.</p> <code>&lt;DeconvolutionChoice.cpu: 'cpu'&gt;</code> <code>psf</code> <code>List[DataArray]</code> <p>List of Point Spread Functions to use for deconvolution. Each of which should be a 3D array. Each PSF can also be provided as a <code>str</code> path, in which case they will be loaded from disk as images.</p> <code>[]</code> <code>psf_num_iter</code> <code>NonNegativeInt</code> <p>Number of iterations to perform in deconvolution</p> <code>10</code> <code>background</code> <code>float | Literal[str, str]</code> <p>Background value to subtract for deconvolution. Only used when <code>decon_processing</code> is set to <code>GPU</code>. This can either be a literal number, \"auto\" which uses the median of the last slice, or \"second_last\" which uses the median of the last slice.</p> <code>0</code>"},{"location":"api/#lls_core.CropParams","title":"lls_core.CropParams","text":"<p>Parameters for the optional cropping step. Note that cropping is performed in the space of the deskewed shape. This is to support the workflow of performing a preview deskew and using that to calculate the cropping coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>roi_list</code> <code>List[Roi]</code> <p>List of regions of interest, each of which must be an <code>N \u00d7 D</code> array, where N is the number of vertices and D the coordinates of each vertex. This can alternatively be provided as a <code>str</code> or <code>Path</code>, or a list of those, in which case each they are interpreted as paths to ImageJ ROI zip files that are read from disk.</p> <code>[]</code> <code>roi_subset</code> <code>List[int]</code> <p>A subset of all the ROIs to process. Each array item should be an index into the ROI list indicating an ROI to include. This allows you to process only a subset of the regions from a ROI file specified using the <code>roi_list</code> parameter. If <code>None</code>, it is assumed that you want to process all ROIs.</p> <code>None</code> <code>z_range</code> <code>Tuple[NonNegativeInt, NonNegativeInt]</code> <p>The range of Z slices to take. All Z slices before the first index or after the last index will be cropped out.</p> <code>None</code>"},{"location":"api/#lls_core.models.results.ImageSlices","title":"lls_core.models.results.ImageSlices","text":"<p>A collection of image slices, which is the main output from deskewing. This holds an iterable of output image slices before they are saved to disk, and provides a <code>save_image()</code> method for this purpose.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray]]]</code> <p>Iterable of result slices. For a given slice, you can access the image data through the <code>slice.data</code> property, which is a numpy-like array.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.ImageSlices.save_image","title":"save_image","text":"<pre><code>save_image()\n</code></pre> <p>Saves result slices to disk</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices","title":"lls_core.models.results.WorkflowSlices","text":"<p>The counterpart of <code>ImageSlices</code>, but for workflow outputs. This is needed because workflows have vastly different outputs that may include regular Python types rather than only image slices.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Tuple[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]], Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]]]</code> <p>Iterable of raw workflow results, the exact nature of which is determined by the author of the workflow. Not typically useful directly, and using he result of <code>.process()</code> is recommended instead.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.WorkflowSlices.extract_preview","title":"extract_preview","text":"<pre><code>extract_preview() -&gt; ArrayLike\n</code></pre> <p>Extracts a single 3D image for previewing purposes</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.process","title":"process","text":"<pre><code>process() -&gt; Iterable[Tuple[RoiIndex, ProcessedWorkflowOutput]]\n</code></pre> <p>Incrementally processes the workflow outputs, and returns both image paths and data frames of the outputs, for image slices and dict/list outputs respectively</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.save","title":"save","text":"<pre><code>save() -&gt; Iterable[Path]\n</code></pre> <p>Processes all workflow outputs and saves them to disk. Images are saved in the format specified in the <code>LatticeData</code>, while other data types are saved as a CSV.</p>"},{"location":"api/#lls_core.models.results.ProcessedWorkflowOutput","title":"lls_core.models.results.ProcessedWorkflowOutput  <code>module-attribute</code>","text":"<pre><code>ProcessedWorkflowOutput = Union[Path, DataFrame]\n</code></pre> <p>The result of a workflow. If this is a <code>Path</code>, then it is the path to an image saved to disk. If a <code>DataFrame</code>, then it contains non-image data returned by your workflow.</p>"},{"location":"api/#lls_core.models.deskew.DefinedPixelSizes","title":"lls_core.models.deskew.DefinedPixelSizes","text":"<p>Like PhysicalPixelSizes, but it's a dataclass, and none of its fields are None</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NonNegativeFloat</code> <p>Size of the X dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Y</code> <code>NonNegativeFloat</code> <p>Size of the Y dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Z</code> <code>NonNegativeFloat</code> <p>Size of the Z dimension of the microscope pixels, in microns.</p> <code>0.3</code>"},{"location":"api/#lls_core.models.deskew.DerivedDeskewFields","title":"lls_core.models.deskew.DerivedDeskewFields","text":"<p>Fields that are automatically calculated based on other fields in DeskewParams. Grouping these together into one model makes validation simpler.</p> <p>Parameters:</p> Name Type Description Default <code>deskew_vol_shape</code> <code>Tuple[int, ...]</code> <p>Dimensions of the deskewed output. This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code> <code>deskew_affine_transform</code> <code>AffineTransform3D</code> <p>Deskewing transformation function. This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code>"},{"location":"api/#lls_core.models.output.SaveFileType","title":"lls_core.models.output.SaveFileType","text":"<p>Choice of File extension to save</p>"},{"location":"cli/","title":"Command Line Interface","text":""},{"location":"cli/#lls-pipeline","title":"lls-pipeline","text":"<p>Usage:</p> <pre><code>lls-pipeline [OPTIONS] [INPUT_IMAGE]\n</code></pre> <p>Options:</p> <pre><code>  [INPUT_IMAGE]                   Path to the image file to read, in a format\n                                  readable by AICSImageIO, for example .tiff\n                                  or .czi\n  --skew [X|Y]                    Axis along which to deskew the image.\n                                  Choices: `{\"X\", \"Y\"}`. These can be provided\n                                  as `str`.   [default: Y]\n  --angle FLOAT                   Angle of deskewing, in degrees, as a float.\n                                  [default: 30.0]\n  --pixel-sizes &lt;FLOAT FLOAT FLOAT&gt;...\n                                  Pixel size of the microscope, in microns.\n                                  This can alternatively be provided as a\n                                  `tuple[float]` of `(Z, Y, X)` This takes\n                                  three arguments, corresponding to the Z, Y\n                                  and X pixel dimensions respectively\n                                  [default: 0.3, 0.1499219272808386,\n                                  0.1499219272808386]\n  --rois PATH                     A list of paths pointing to regions of\n                                  interest to crop to, in ImageJ format.\n  --roi-indices INTEGER           A subset of all the ROIs to process. Each\n                                  array item should be an index into the ROI\n                                  list indicating an ROI to include. This\n                                  allows you to process only a subset of the\n                                  regions from a ROI file specified using the\n                                  `roi_list` parameter. If `None`, it is\n                                  assumed that you want to process all ROIs.\n  --z-start INTEGER               The index of the first Z slice to use. All\n                                  prior Z slices will be discarded.\n  --z-end INTEGER                 The index of the last Z slice to use. The\n                                  selected index and all subsequent Z slices\n                                  will be discarded. Defaults to the last z\n                                  index of the image.\n  --deconvolution / --disable-deconvolution\n                                  [default: disable-deconvolution]\n  --decon-processing [cuda_gpu|opencl_gpu|cpu]\n                                  Hardware to use to perform the\n                                  deconvolution. Choices: `{\"cuda_gpu\",\n                                  \"opencl_gpu\", \"cpu\"}`. Can be provided as\n                                  `str`.   [default: cpu]\n  --psf PATH                      One or more paths pointing to point spread\n                                  functions to use for deconvolution. Each\n                                  file should in a standard image format\n                                  (.czi, .tiff etc), containing a 3D image\n                                  array. This option can be used multiple\n                                  times to provide multiple PSF files.\n  --psf-num-iter INTEGER          Number of iterations to perform in\n                                  deconvolution   [default: 10]\n  --background TEXT               Background value to subtract for\n                                  deconvolution. Only used when\n                                  `decon_processing` is set to `GPU`. This can\n                                  either be a literal number, \"auto\" which\n                                  uses the median of the last slice, or\n                                  \"second_last\" which uses the median of the\n                                  last slice.   [default: 0]\n  --time-start INTEGER            Index of the first time slice to use\n                                  (inclusive). Defaults to the first time\n                                  index of the image.  [default: 0]\n  --time-end INTEGER              Index of the first time slice to use\n                                  (exclusive). Defaults to the last time index\n                                  of the image.\n  --channel-start INTEGER         Index of the first channel slice to use\n                                  (inclusive). Defaults to the first channel\n                                  index of the image.  [default: 0]\n  --channel-end INTEGER           Index of the first channel slice to use\n                                  (exclusive). Defaults to the last channel\n                                  index of the image.\n  --save-dir PATH                 The directory where the output data will be\n                                  saved. This can be specified as a `str` or\n                                  `Path`.\n  --save-name TEXT                The filename that will be used for output\n                                  files. This should not contain a leading\n                                  directory or file extension. The final\n                                  output files will have additional elements\n                                  added to the end of this prefix to indicate\n                                  the region of interest, channel, timepoint,\n                                  file extension etc.\n  --save-type [h5|tiff]           The data type to save the result as. This\n                                  will also be used to determine the file\n                                  extension of the output files. Choices:\n                                  `{\"h5\", \"tiff\"}`. Choices can alternatively\n                                  be specifed as `str`, for example `'tiff'`.\n                                  [default: h5]\n  --workflow PATH                 Path to a Napari Workflow file, in YAML\n                                  format. If provided, the configured\n                                  desekewing processing will be added to the\n                                  chosen workflow.\n  --json-config PATH              Path to a JSON file from which parameters\n                                  will be read.\n  --yaml-config PATH              Path to a YAML file from which parameters\n                                  will be read.\n  --show-schema / --no-show-schema\n                                  If provided, image processing will not be\n                                  performed, and instead a JSON document\n                                  outlining the JSON/YAML options will be\n                                  printed to stdout. This can be used to\n                                  assist with writing a config file for use\n                                  with the --json-config and --yaml-config\n                                  options.  [default: no-show-schema]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"development/","title":"Development","text":""},{"location":"development/#structure","title":"Structure","text":"<p>The repo is divided into two python packages: <code>core</code> (the core processing and CLI) and <code>plugin</code> (the Napari GUI). Within each directory is:</p> <ul> <li>The python package</li> <li><code>pyproject.toml</code>, the package metadata such as the dependencies, and </li> <li><code>tests/</code>, which contains the tests</li> </ul>"},{"location":"development/#installation","title":"Installation","text":"<p>For local development, first clone the repo and then run the following from the repository root:</p> <pre><code>pip install -e core -e plugin\n</code></pre>"},{"location":"development/#technologies","title":"Technologies","text":""},{"location":"development/#pydantic","title":"Pydantic","text":"<p>Used for defining the parameter sets, performing parameter validation and conversion. These models live in <code>core/lls_core/models</code>. Note, <code>lls_core</code> uses Pydantic 1.X, which has a different API to Pydantic 2.X. You can find relevant documentation here: https://docs.pydantic.dev/1.10/</p>"},{"location":"development/#xarray","title":"Xarray","text":"<p>Used for all image representations, where they are treated as multidimensional arrays with dimensional labels (X, Y, Z etc). Refer to: https://xarray.pydata.org/.</p>"},{"location":"development/#typer","title":"Typer","text":"<p>The CLI is defined using Typer: https://typer.tiangolo.com/.</p>"},{"location":"development/#magicgui-and-magicclass","title":"magicgui and magicclass","text":"<p>These packages are used to define the GUI, which you can find in <code>plugin/napari_lattice</code>. <code>magicclass</code> builds on <code>magicgui</code> by providing the <code>@magicclass</code> decorator which turns a Python class into a GUI.</p>"},{"location":"development/#adding-a-new-parameter","title":"Adding a new parameter","text":"<p>Whenever a new parameter is added, the following components need to be updated:</p> <ul> <li>Add the parameter to the Pydantic models</li> <li>Add the parameter to the CLI (<code>core/lls_core/cmds/__main__.py</code>), and define mapping between CLI and Pydantic using the <code>CLI_PARAM_MAP</code></li> <li>Add the field to the GUI in <code>plugin/napari_lattice/fields.py</code></li> <li>Define the new processing logic in <code>core/lls_core/models/lattice_data.py</code></li> </ul> <p>An example of this can be found in this commit: https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/pull/47/commits/16b28fec307f19e73b8d55e677621082037b2710.</p>"},{"location":"development/#testing","title":"Testing","text":"<p>The tests are run using pytest. To install the testing dependencies, use <code>pip install -e 'core[testing]' -e 'plugin[testing]'</code> Since there are two separate packages, you will have to specify the location of each test directory. To run all the tests, use <code>pytest core/tests/ plugin/tests</code> from the root directory.</p>"},{"location":"installation/","title":"Installation","text":"<p>It is advisable but not required that you install Napari Lattice packages inside a conda environment. This is because conda makes it much easier to install system complex dependencies.</p> <p>To install the core package, which includes the Python library and command line interface:</p> <pre><code>pip install lls-core\n</code></pre> <p>To install the Napari plugin:</p> <pre><code>pip install napari-lattice\n</code></pre>"},{"location":"installation/#cuda-deconvolution","title":"CUDA Deconvolution","text":"<p>If you have access to a CUDA-compatible GPU, you can enable deconvolution using <code>pycudadecon</code>.</p> <p>If you're using conda (or micromamba etc), you can run the following:</p> <pre><code>conda install -c conda-forge pycudadecon\n</code></pre> <p>Otherwise, you will have to manually ensure the systems dependencies are installed, and then:</p> <pre><code>pip install lls-core[deconvolution]\n</code></pre>"},{"location":"installation/#development-versions","title":"Development Versions","text":"<p>To install the development version of <code>lls-core</code>:</p> <pre><code>pip install git+https://github.com/BioimageAnalysisCoreWEHI/napari_lattice.git#subdirectory=core\n</code></pre> <p>For <code>napari-lattice</code>:</p> <pre><code>pip install git+https://github.com/BioimageAnalysisCoreWEHI/napari_lattice.git#subdirectory=plugin\n</code></pre>"}]}