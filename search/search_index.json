{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>../README.md</p>"},{"location":"api/","title":"Python Usage","text":""},{"location":"api/#python-usage","title":"Python Usage","text":""},{"location":"api/#introduction","title":"Introduction","text":"<p>The image processing workflow can also be controlled via Python API.</p> <p>To do so, first define the parameters:</p> <pre><code>from lls_core import LatticeData\n\nparams = LatticeData(\n  input_image=\"/path/to/some/file.tiff\",\n  save_dir=\"/path/to/output\"\n)\n</code></pre> <p>Then save the result to disk: </p><pre><code>params.save()\n</code></pre> <p>Or work with the images in memory: </p><pre><code>for slice in params.process():\n    pass\n</code></pre> <p>Other more advanced options are listed below.</p>"},{"location":"api/#cropping","title":"Cropping","text":"<p>Cropping functionality can be enabled by setting the <code>crop</code> parameter:</p> <pre><code>from lls_core import LatticeData, CropParams \n\nparams = LatticeData(\n  input_image=\"/path/to/some/file.tiff\",\n  save_dir=\"/path/to/output\",\n  crop=CropParams(\n    roi_list=[\"/path/to/roi.zip\"]\n  )\n)\n</code></pre> <p>Other more advanced options are listed below.</p>"},{"location":"api/#type-checking","title":"Type Checking","text":"<p>Because of Pydantic idiosyncrasies, the <code>LatticeData</code> constructor can accept more data types than the type system realises.  For example, <code>input_image=\"/some/path\"</code> like we used above is not considered correct, because ultimately the input image has to become an <code>xarray</code> (aka <code>DataArray</code>). You can solve this in three ways.</p> <p>The first is to use the types precisely as defined. In this case, we might define the parameters \"correctly\" (if verbosely) like this:</p> <pre><code>from lls_core import LatticeData\nfrom aicsimageio import AICSImage\nfrom pathlib import Path\n\nparams = LatticeData(\n  input_image=AICSImage(\"/path/to/some/file.tiff\").xarray_dask_data(),\n  save_dir=Path(\"/path/to/output\")\n)\n</code></pre> <p>The second is to use <code>LatticeData.parse_obj</code>, which takes a dictionary of options and allows incorrect types:</p> <pre><code>params = LatticeData.parse_obj({\n  \"input_image\": \"/path/to/some/file.tiff\",\n  \"save_dir\": \"/path/to/output\"\n})\n</code></pre> <p>Finally, if you're using MyPy, you can install the pydantic plugin, which solves this problem via the <code>init_typed = False</code> option.</p>"},{"location":"api/#api-docs","title":"API Docs","text":""},{"location":"api/#lls_core.LatticeData","title":"lls_core.LatticeData","text":"<p>Parameters for the entire deskewing process, including outputs and optional steps such as deconvolution. This is the recommended entry point for Python users: construct an instance of this class, and then perform the processing using methods.</p> <p>Note that none of this class's methods have any parameters: all parameters are class fields for validation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>DataArray</code> <p>A 3-5D array containing the image data. Can be anything convertible to an Xarray, including a <code>dask.array</code> or <code>numpy.ndarray</code>. Can also be provided as a <code>str</code>, in which case it must indicate the path to an image to load from disk.</p> <code>None</code> <code>skew</code> <code>DeskewDirection</code> <p>Axis along which to deskew the image. Choices: <code>{\"X\", \"Y\"}</code>. These can be provided as <code>str</code>.</p> <code>&lt;DeskewDirection.Y: 2&gt;</code> <code>angle</code> <code>float</code> <p>Angle of deskewing, in degrees, as a float.</p> <code>30.0</code> <code>physical_pixel_sizes</code> <code>DefinedPixelSizes</code> <p>Pixel size of the microscope, in microns. This can alternatively be provided as a <code>tuple[float]</code> of <code>(Z, Y, X)</code></p> <code>None</code> <code>derived</code> <code>DerivedDeskewFields</code> <p>Refer to the <code>DerivedDeskewFields</code> docstring</p> <code>None</code> <code>save_dir</code> <code>DirectoryPath</code> <p>The directory where the output data will be saved. This can be specified as a <code>str</code> or <code>Path</code>.</p> <code>None</code> <code>save_suffix</code> <code>str</code> <p>The filename suffix that will be used for output files. This will be added as a suffix to the input file name if the input image was specified using a file name. If the input image was provided as an in-memory object, the <code>save_name</code> field should instead be specified.</p> <code>'_deskewed'</code> <code>save_name</code> <code>str</code> <p>The filename that will be used for output files. This should not contain a leading directory or file extension. The final output files will have additional elements added to the end of this prefix to indicate the region of interest, channel, timepoint, file extension etc.</p> <code>None</code> <code>save_type</code> <code>SaveFileType</code> <p>The data type to save the result as. This will also be used to determine the file extension of the output files. Choices: <code>{\"h5\", \"tiff\"}</code>. Choices can alternatively be specifed as <code>str</code>, for example <code>'tiff'</code>.</p> <code>&lt;SaveFileType.h5: 'h5'&gt;</code> <code>time_range</code> <code>range</code> <p>The range of times to process. This defaults to all time points in the image array.</p> <code>None</code> <code>channel_range</code> <code>range</code> <p>The range of channels to process. This defaults to all time points in the image array.</p> <code>None</code> <code>deconvolution</code> <code>DeconvolutionParams | None</code> <p>Parameters associated with the deconvolution. If this is None, then deconvolution is disabled</p> <code>None</code> <code>crop</code> <code>CropParams | None</code> <p>Cropping parameters. If this is None, then cropping is disabled</p> <code>None</code> <code>workflow</code> <code>Workflow | None</code> <p>If defined, this is a workflow to add lightsheet processing onto</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>If true, show progress bars</p> <code>True</code>"},{"location":"api/#lls_core.LatticeData.process","title":"process","text":"<pre><code>process() -&gt; ImageSlices\n</code></pre> <p>Execute the processing and return the result. This will not execute the attached workflow.</p>"},{"location":"api/#lls_core.LatticeData.process_workflow","title":"process_workflow","text":"<pre><code>process_workflow() -&gt; WorkflowSlices\n</code></pre> <p>Runs the workflow on each slice and returns the workflow results</p>"},{"location":"api/#lls_core.LatticeData.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Apply the processing, and saves the results to disk. Results can be found in <code>save_dir</code>.</p>"},{"location":"api/#lls_core.DeconvolutionParams","title":"lls_core.DeconvolutionParams","text":"<p>Parameters for the optional deconvolution step</p> <p>Parameters:</p> Name Type Description Default <code>decon_processing</code> <code>DeconvolutionChoice</code> <p>Hardware to use to perform the deconvolution. Choices: <code>{\"cuda_gpu\", \"opencl_gpu\", \"cpu\"}</code>. Can be provided as <code>str</code>.</p> <code>&lt;DeconvolutionChoice.cpu: 'cpu'&gt;</code> <code>psf</code> <code>List[DataArray]</code> <p>List of Point Spread Functions to use for deconvolution. Each of which should be a 3D array. Each PSF can also be provided as a <code>str</code> path, in which case they will be loaded from disk as images.</p> <code>[]</code> <code>decon_num_iter</code> <code>NonNegativeInt</code> <p>Number of iterations to perform in deconvolution</p> <code>10</code> <code>background</code> <code>float | Literal[str, str]</code> <p>Background value to subtract for deconvolution. Only used when <code>decon_processing</code> is set to <code>GPU</code>. This can either be a literal number, \"auto\" which uses the median of the last slice, or \"second_last\" which uses the median of the last slice.</p> <code>0</code>"},{"location":"api/#lls_core.CropParams","title":"lls_core.CropParams","text":"<p>Parameters for the optional cropping step. Note that cropping is performed in the space of the deskewed shape. This is to support the workflow of performing a preview deskew and using that to calculate the cropping coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>roi_list</code> <code>List[Roi]</code> <p>List of regions of interest, each of which must be an <code>N \u00d7 D</code> array, where N is the number of vertices and D the coordinates of each vertex. This can alternatively be provided as a <code>str</code> or <code>Path</code>, or a list of those, in which case each they are interpreted as paths to ImageJ ROI zip files that are read from disk.</p> <code>[]</code> <code>roi_subset</code> <code>List[int]</code> <p>A subset of all the ROIs to process. Each list item should be an index into the ROI list indicating an ROI to include. This allows you to process only a subset of the regions from a ROI file specified using the <code>roi_list</code> parameter. If <code>None</code>, it is assumed that you want to process all ROIs.</p> <code>None</code> <code>z_range</code> <code>Tuple[NonNegativeInt, NonNegativeInt]</code> <p>The range of Z slices to take as a tuple of the form <code>(first, last)</code>. All Z slices before the first index or after the last index will be cropped out.</p> <code>None</code>"},{"location":"api/#lls_core.models.results.ImageSlices","title":"lls_core.models.results.ImageSlices","text":"<p>A collection of image slices, which is the main output from deskewing. This holds an iterable of output image slices before they are saved to disk, and provides a <code>save_image()</code> method for this purpose.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray]]]</code> <p>Iterable of result slices. For a given slice, you can access the image data through the <code>slice.data</code> property, which is a numpy-like array.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.ImageSlices.roi_previews","title":"roi_previews","text":"<pre><code>roi_previews() -&gt; Iterable[ArrayLike]\n</code></pre> <p>Extracts a single 3D image for each ROI</p>"},{"location":"api/#lls_core.models.results.ImageSlices.save_image","title":"save_image","text":"<pre><code>save_image()\n</code></pre> <p>Saves result slices to disk</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices","title":"lls_core.models.results.WorkflowSlices","text":"<p>The counterpart of <code>ImageSlices</code>, but for workflow outputs. This is needed because workflows have vastly different outputs that may include regular Python types rather than only image slices.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Tuple[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]], Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]]]</code> <p>Iterable of raw workflow results, the exact nature of which is determined by the author of the workflow. Not typically useful directly, and using he result of <code>.process()</code> is recommended instead.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.WorkflowSlices.process","title":"process","text":"<pre><code>process() -&gt; Iterable[ProcessedWorkflowOutput]\n</code></pre> <p>Incrementally processes the workflow outputs, and returns both image paths and data frames of the outputs, for image slices and dict/list outputs respectively</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.roi_previews","title":"roi_previews","text":"<pre><code>roi_previews() -&gt; Iterable[NDArray]\n</code></pre> <p>Extracts a single 3D image for each ROI</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.save","title":"save","text":"<pre><code>save() -&gt; Iterable[Path]\n</code></pre> <p>Processes all workflow outputs and saves them to disk. Images are saved in the format specified in the <code>LatticeData</code>, while other data types are saved as a CSV. Remember to call <code>list()</code> on this to exhaust the generator and run the computation.</p>"},{"location":"api/#lls_core.models.results.ProcessedWorkflowOutput","title":"lls_core.models.results.ProcessedWorkflowOutput","text":"<p>Result class for one single workflow output, after it has been processed.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>An enumeration.</p> <code>None</code> <code>roi_index</code> <code>int | None</code> <p>An enumeration.</p> <code>None</code> <code>data</code> <code>Path | DataFrame</code> <p>An enumeration.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>An enumeration.</p> required"},{"location":"api/#lls_core.models.results.ProcessedWorkflowOutput.save","title":"save","text":"<pre><code>save() -&gt; Path\n</code></pre> <p>Puts this artifact on disk by saving any <code>DataFrame</code> to CSV, and returning the path to the image or CSV</p>"},{"location":"api/#lls_core.models.deskew.DefinedPixelSizes","title":"lls_core.models.deskew.DefinedPixelSizes","text":"<p>Like PhysicalPixelSizes, but it's a dataclass, and none of its fields are None</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NonNegativeFloat</code> <p>Size of the X dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Y</code> <code>NonNegativeFloat</code> <p>Size of the Y dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Z</code> <code>NonNegativeFloat</code> <p>Size of the Z dimension of the microscope pixels, in microns.</p> <code>0.3</code>"},{"location":"api/#lls_core.models.deskew.DerivedDeskewFields","title":"lls_core.models.deskew.DerivedDeskewFields","text":"<p>Fields that are automatically calculated based on other fields in DeskewParams. Grouping these together into one model makes validation simpler.</p> <p>Parameters:</p> Name Type Description Default <code>deskew_vol_shape</code> <code>Tuple[int, ...]</code> <p>Dimensions of the deskewed output. This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code> <code>deskew_affine_transform</code> <code>AffineTransform3D</code> <p>Deskewing affine transformation matrix (in xyz order for OpenCL). This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code> <code>deskew_affine_transform_zyx</code> <code>ndarray</code> <p>Deskewing affine transformation matrix (zyx order). This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code>"},{"location":"api/#lls_core.models.output.SaveFileType","title":"lls_core.models.output.SaveFileType","text":"<p>Choice of File extension to save</p>"},{"location":"cli/","title":"Command Line Interface","text":""},{"location":"cli/#command-line-interface","title":"Command Line Interface","text":""},{"location":"cli/#lls-pipeline","title":"lls-pipeline","text":"<p>Usage:</p> <pre><code>lls-pipeline [OPTIONS] [INPUT_IMAGE]\n</code></pre> <p>Options:</p> <pre><code>  [INPUT_IMAGE]                   Path to the image file to read, in a format\n                                  readable by AICSImageIO, for example .tiff\n                                  or .czi\n  --skew [X|Y]                    Axis along which to deskew the image.\n                                  Choices: `{\"X\", \"Y\"}`. These can be provided\n                                  as `str`.   \\[default: Y]\n  --angle FLOAT                   Angle of deskewing, in degrees, as a float.\n                                  \\[default: 30.0]\n  --physical-pixel-sizes &lt;FLOAT FLOAT FLOAT&gt;...\n                                  Pixel size of the microscope, in microns.\n                                  This can alternatively be provided as a\n                                  `tuple[float]` of `(Z, Y, X)` This takes\n                                  three arguments, corresponding to the Z, Y\n                                  and X pixel dimensions respectively\n                                  \\[default: 0.3, 0.1499219272808386,\n                                  0.1499219272808386]\n  --roi-list PATH                 List of regions of interest, each of which\n                                  must be an `N \u00d7 D` array, where N is the\n                                  number of vertices and D the coordinates of\n                                  each vertex. This can alternatively be\n                                  provided as a `str` or `Path`, or a list of\n                                  those, in which case each they are\n                                  interpreted as paths to ImageJ ROI zip files\n                                  that are read from disk.\n  --roi-subset INTEGER            A subset of all the ROIs to process. Each\n                                  list item should be an index into the ROI\n                                  list indicating an ROI to include. This\n                                  allows you to process only a subset of the\n                                  regions from a ROI file specified using the\n                                  `roi_list` parameter. If `None`, it is\n                                  assumed that you want to process all ROIs.\n  --z-range &lt;INTEGER INTEGER&gt;...  The range of Z slices to take as a tuple of\n                                  the form `(first, last)`. All Z slices\n                                  before the first index or after the last\n                                  index will be cropped out.\n  --deconvolution / --disable-deconvolution\n                                  \\[default: disable-deconvolution]\n  --decon-processing [cuda_gpu|opencl_gpu|cpu]\n                                  Hardware to use to perform the\n                                  deconvolution. Choices: `{\"cuda_gpu\",\n                                  \"opencl_gpu\", \"cpu\"}`. Can be provided as\n                                  `str`.   \\[default: cpu]\n  --psf PATH                      List of Point Spread Functions to use for\n                                  deconvolution. Each of which should be a 3D\n                                  array. Each PSF can also be provided as a\n                                  `str` path, in which case they will be\n                                  loaded from disk as images.\n  --decon-num-iter INTEGER        Number of iterations to perform in\n                                  deconvolution   \\[default: 10]\n  --background TEXT               Background value to subtract for\n                                  deconvolution. Only used when\n                                  `decon_processing` is set to `GPU`. This can\n                                  either be a literal number, \"auto\" which\n                                  uses the median of the last slice, or\n                                  \"second_last\" which uses the median of the\n                                  last slice.   \\[default: 0]\n  --time-range &lt;INTEGER INTEGER&gt;...\n                                  The range of times to process. This defaults\n                                  to all time points in the image array.\n  --channel-range &lt;INTEGER INTEGER&gt;...\n                                  The range of channels to process. This\n                                  defaults to all time points in the image\n                                  array.\n  --save-dir PATH                 The directory where the output data will be\n                                  saved. This can be specified as a `str` or\n                                  `Path`.\n  --save-name TEXT                The filename that will be used for output\n                                  files. This should not contain a leading\n                                  directory or file extension. The final\n                                  output files will have additional elements\n                                  added to the end of this prefix to indicate\n                                  the region of interest, channel, timepoint,\n                                  file extension etc.\n  --save-type [h5|tiff]           The data type to save the result as. This\n                                  will also be used to determine the file\n                                  extension of the output files. Choices:\n                                  `{\"h5\", \"tiff\"}`. Choices can alternatively\n                                  be specifed as `str`, for example `'tiff'`.\n                                  \\[default: h5]\n  --workflow PATH                 If defined, this is a workflow to add\n                                  lightsheet processing onto\n  --json-config PATH              Path to a JSON file from which parameters\n                                  will be read.\n  --yaml-config PATH              Path to a YAML file from which parameters\n                                  will be read.\n  --show-schema / --no-show-schema\n                                  If provided, image processing will not be\n                                  performed, and instead a JSON document\n                                  outlining the JSON/YAML options will be\n                                  printed to stdout. This can be used to\n                                  assist with writing a config file for use\n                                  with the --json-config and --yaml-config\n                                  options.  \\[default: no-show-schema]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"development/","title":"Development","text":""},{"location":"development/#development","title":"Development","text":""},{"location":"development/#structure","title":"Structure","text":"<p>The repo is divided into two python packages: <code>core</code> (the core processing and CLI) and <code>plugin</code> (the Napari GUI). Within each directory is:</p> <ul> <li>The python package</li> <li><code>pyproject.toml</code>, the package metadata such as the dependencies, and </li> <li><code>tests/</code>, which contains the tests</li> </ul>"},{"location":"development/#installation","title":"Installation","text":"<p>For local development, first clone the repo and then run the following from the repository root: </p><pre><code>pip install -e core -e plugin\n</code></pre>"},{"location":"development/#technologies","title":"Technologies","text":""},{"location":"development/#pydantic","title":"Pydantic","text":"<p>Used for defining the parameter sets, performing parameter validation and conversion. These models live in <code>core/lls_core/models</code>. Note, <code>lls_core</code> uses Pydantic 1.X, which has a different API to Pydantic 2.X. You can find relevant documentation here: https://docs.pydantic.dev/1.10/</p>"},{"location":"development/#xarray","title":"Xarray","text":"<p>Used for all image representations, where they are treated as multidimensional arrays with dimensional labels (X, Y, Z etc). Refer to: https://xarray.pydata.org/.</p>"},{"location":"development/#typer","title":"Typer","text":"<p>The CLI is defined using Typer: https://typer.tiangolo.com/.</p>"},{"location":"development/#magicgui-and-magicclass","title":"magicgui and magicclass","text":"<p>These packages are used to define the GUI, which you can find in <code>plugin/napari_lattice</code>. <code>magicclass</code> builds on <code>magicgui</code> by providing the <code>@magicclass</code> decorator which turns a Python class into a GUI.</p>"},{"location":"development/#dev-workflows","title":"Dev Workflows","text":""},{"location":"development/#adding-a-new-parameter","title":"Adding a new parameter","text":"<p>Whenever a new parameter is added, the following components need to be updated:</p> <ul> <li>Add the parameter to the Pydantic models</li> <li>Add the parameter to the CLI (<code>core/lls_core/cmds/__main__.py</code>), and define mapping between CLI and Pydantic using the <code>CLI_PARAM_MAP</code></li> <li>Add the field to the GUI in <code>plugin/napari_lattice/fields.py</code></li> <li>Define the new processing logic in <code>core/lls_core/models/lattice_data.py</code></li> </ul> <p>An example of this can be found in this commit: https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/pull/47/commits/16b28fec307f19e73b8d55e677621082037b2710.</p>"},{"location":"development/#adding-a-new-image-reader","title":"Adding a new image reader","text":"<p>Currently there aren't image reader classes. Instead, we currently have a pydantic validator that converts the image from a path to an array, or from an array into an xarray. A new format could be implemented in this validator: https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/blob/b33cc4ca5fe0fb89d730cefdbe3169f984f1fe89/core/lls_core/models/deskew.py#L176-L202</p>"},{"location":"development/#adding-a-new-image-writer","title":"Adding a new image writer","text":"<ol> <li> <p>Create a new writer which inherits from the <code>lls_core.writers.Writer</code> class, and implements its <code>write_slice</code> method:</p> </li> <li> <p>Add a new option to the <code>SaveFileType</code> enum</p> </li> <li> <p>Then, return the correct writer class based on the enum value.</p> </li> </ol>"},{"location":"development/#lls_core.writers.Writer.write_slice","title":"lls_core.writers.Writer.write_slice  <code>abstractmethod</code>","text":"<pre><code>write_slice(slice: ProcessedSlice[ArrayLike])\n</code></pre> <p>Writes a 3D image slice</p>"},{"location":"development/#testing","title":"Testing","text":"<p>The tests are run using pytest. To install the testing dependencies, use <code>pip install -e 'core[testing]' -e 'plugin[testing]'</code> Since there are two separate packages, you will have to specify the location of each test directory. To run all the tests, use <code>pytest core/tests/ plugin/tests</code> from the root directory.</p>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Docs are built with mkdocs.</p> <p>To modify the docs, you need the docs dependencies, so clone the repo and then:</p> <pre><code>pip install -e 'core[docs]'\n</code></pre> <p>The key files are:</p> <pre><code>* `mkdocs.yml`, which is the main config file for mkdocs, and\n* `docs/` which is a directory containing markdown files. Each new file that gets added there will create a new page in the website.\n</code></pre> <p>Some useful <code>mkdocs</code> commands:</p> <ul> <li><code>mkdocs serve</code> runs the development server which hosts the docs on a local web server. Any changes to your markdown files will be reflected in this server, although you sometimes have to restart the server if you make a change to configuration</li> <li><code>mkdocs gh-deploy</code> builds the docs, and pushes them to GitHub Pages. This updates the docs at https://bioimageanalysiscorewehi.github.io/napari_lattice/.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This section will guide you through the installation of napari-lattice using conda. The commands can be copied by clicking the <code>copy</code> button at the right side of the respective code blocks below.</p>"},{"location":"installation/#recommended-installation","title":"Recommended Installation","text":"<p>We recommend the installation of Miniconda as is a minimal version of Anaconda Distribution. </p> Tip <p>If you can install <code>mamba</code> in your environment, the conda installation commands will be much faster. Once configured correctly, just replace <code>conda</code> with <code>mamba</code> in the commands below.</p> <p>First, create a new conda environment:</p> <pre><code>conda create -n napari-lattice -c conda-forge \"python==3.10\" uv pycudadecon \"numpy&lt;2\"\n</code></pre> <p>Info</p> <p>We include <code>pycudadecon</code> to enable GPU accelerated deconvolution.</p> <p>Activate that environment:</p> <pre><code>conda activate napari-lattice\n</code></pre> <p>Then use <code>uv</code> to quickly install the napari-lattice suite using the following 2 commands:</p> <pre><code>uv pip install lls-core napari-lattice\n</code></pre> <pre><code>uv pip install --upgrade aicsimageio \"napari==0.5.5\" \"numpy&lt;2\"\n</code></pre>"},{"location":"installation/#cli-only","title":"CLI only","text":"<p>If you do not need to use napari, then you can install just the command line interface only, which has all the features</p> <pre><code>conda create -n napari-lattice -c conda-forge \"python==3.10\" uv pycudadecon \"numpy&lt;2\"\n</code></pre> <pre><code>uv pip install lls-core\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>To install the development version of <code>lls-core</code>, create the <code>napari-lattice</code> environment as above, but instead of installing from pip in the last step:</p> <pre><code>git clone https://github.com/BioimageAnalysisCoreWEHI/napari_lattice.git\ncd napari_lattice\nuv pip install -e core -e plugin\nuv pip install napari --upgrade \"numpy&lt;2\"\n</code></pre>"},{"location":"workflow/","title":"Workflows","text":""},{"location":"workflow/#workflows","title":"Workflows","text":"<p><code>lls_core</code> supports integration with <code>napari-workflows</code>. The advantage of this is that you can design a multi-step automated workflow that uses <code>lls_core</code> as the pre-processing step.</p>"},{"location":"workflow/#building-a-workflow","title":"Building a Workflow","text":"<p>You can design your workflow via GUI using <code>napari-assistant</code>, or directly in the YAML format.</p> <p>When building your workflow with Napari Assistant, you are actually building a template that will be applied to future images. For this reason, you need to rename your input layer to <code>deskewed_image</code>, since this is the exact value that the <code>lls_core</code> step produces.</p> <p>If you want to use YAML, you also have to make sure that the first workflow step to run takes <code>deskewed_image</code> as an input. For example:</p> <pre><code>!!python/object:napari_workflows._workflow.Workflow\n_tasks:\n  median: !!python/tuple\n  - !!python/name:pyclesperanto_prototype.median_sphere ''\n  - deskewed_image\n  - null\n  - 2\n  - 2\n  - 2\n</code></pre> <p>Workflows are run once for each 3D slice of the image. In other words, the workflow is run separately for each timepoint, for each channel, for each region of interest (if cropping is enabled). This means that you should design your workflow expecting that <code>deskewed_image</code> is an exactly 3D array.</p> <p>If you want to define your own custom functions, you can do so in a <code>.py</code> file in the same directory as the workflow <code>.yml</code> file.  These will be imported before the workflow is executed.</p>"},{"location":"workflow/#running-a-workflow","title":"Running a Workflow","text":"<p>The <code>--workflow</code> command-line flag, the <code>LatticeData(workflow=)</code> Python parameter, and the Workflow tab of the plugin can be used to specify the path to a workflow <code>.yml</code> file .</p> <p>If you're using the Python interface, you need to use <code>LatticeData.process_workflow()</code> rather than <code>.process()</code>. </p>"},{"location":"workflow/#outputs","title":"Outputs","text":"<p><code>lls_core</code> supports workflows that have exactly one \"leaf\" task. This is defined as a task that is not used by any other tasks. In other works, it's the final task of the workflow.</p> <p>If you want multiple return values, this task can return a tuple of values. Each of these values must be:</p> <ul> <li>An array, in which case it is treated as an image slice</li> <li>A <code>dict</code>, in which case it is treated as a single row of a data frame whose columns are the keys of the <code>dict</code></li> <li>A <code>list</code>, in which case it is treated as a single row of a data frame whose columns are unnamed</li> </ul> <p>Then, each slice is combined at the end. Image slices are stacked together into their original dimensionality, and data frame rows are stacked into a data frame with one row per channel, timepoint and ROI.</p>"},{"location":"miscellaneous/","title":"Defining ROIs for cropping","text":""},{"location":"miscellaneous/#zeiss-lattice-lightsheet-7","title":"Zeiss Lattice Lightsheet 7","text":"<p>When using the Zeiss LLS7, at the end of every acquisition a maximum intensity projection (MIP) image is created. This can be used for defining the ROIs for cropping. However, the ROIs need to be rotated by 90 degrees before it can be used in napari-lattice. </p> <p>There are two ways around this in Fiji:</p>"},{"location":"miscellaneous/#1-rotate-rois-in-fiji","title":"1. Rotate ROIs in Fiji","text":"<ul> <li>Open the LLS7 MIP in Fiji. If you have multiple wells, they will appear as different series.</li> </ul> <ul> <li>Draw ROIs on the MIP</li> </ul> <ul> <li>Save the ROIs in a folder</li> <li>Download this Fiji macro. You will need plugins: BIOP and MorpholibJ.</li> <li>To run this, drag and drop onto Fiji. Once it opens, you can either click <code>F5</code> or <code>Run -&gt; Run</code> in the menu on the macro window.</li> <li>You will get the following window.</li> </ul> <pre><code>* `Choose LLS7 image`: Enter path to the image\n* `Choose ROI Manager file`: ROI Manager file created above with areas to crop\n* `ROI Save directory`: Location to save the modified ROIs\n</code></pre> <ul> <li>If the MIP has multiple series, then you will get the prompt below.</li> </ul> <ul> <li>Specify the series number to process. Note that you should specify the ROI file for the corresponding series when running this macro.</li> <li>This will process the ROI Manager file and save it in the specified directory with <code>_corrected</code> suffix.</li> <li>This ROI file can be used in the napari-lattice workflows.</li> </ul>"},{"location":"miscellaneous/#2-rotate-image-and-then-define-roi","title":"2. Rotate image and then define ROI","text":"<p>Alternatively, you can rotate the image, draw ROIs and save the ROI Manager file.</p> <ul> <li>Open the MIP image in Fiji</li> <li> <p>Go to Image -&gt; Transform -&gt; Rotate 90 degrees left</p> <p></p> </li> <li> <p>Wait for the Image to be rotated.</p> </li> <li>Once that is finished, draw ROIs using the rectangle tool. </li> <li>Add each ROI to the ROI Manager.</li> <li>Save the ROI Manager as a zip file. </li> <li>This ROI file can now be imported into napari-lattice workflows.</li> </ul>"},{"location":"napari_plugin/","title":"Initializing the plugin","text":""},{"location":"napari_plugin/#initializing-the-plugin","title":"Initializing the plugin","text":""},{"location":"napari_plugin/#starting-the-plugin","title":"Starting the plugin","text":"<p>The napari plugin has been redesigned in the newer version. To activate the plugin, activate the <code>napari-lattice</code> environment in your terminal and type <code>napari</code> in the console. The plugin is under <code>Lattice Lightsheet Analysis</code></p> <p></p> <p>The plugin should appear on the right side. You may have to resize the window.</p> <p></p> <p>The functionalities with napari-lattice have been separated out into tabs:</p> <p></p> <p>If its configured correctly, you should see a green tick next to the tab name, else you will see a red cross.</p> <p></p> <p>To load an image, drag and drop it into napari. You can get some sample data here. We are using <code>RBC_tiny.czi</code> as an example here.</p> <p></p> <p>Info</p> <p>When opening a file, if you get a pop-up asking for preferred reader with <code>czi</code> files, select napari-aicsimageio</p>"},{"location":"napari_plugin/#configuration","title":"Configuration","text":"<p>To configure any parameters, you can change the settings here:</p> <p></p> <p>To use the plugin, click on <code>Using the Plugin</code> on the left menu.</p>"},{"location":"napari_plugin/plugin_usage/","title":"Using the Plugin","text":""},{"location":"napari_plugin/plugin_usage/#plugin-usage","title":"Plugin Usage","text":"<p>Click on the tabs below to view the corresponding functionality.</p> DeskewingQuick DeskewDeconvolutionCroppingWorkflowOutput (Saving files) <p>To use the specific image for processing, you will have to select it under the <code>Image Layer(s) to Deskew</code> box on the right. Here, we will click on <code>RBC_tiny</code>. As its a czi file it should read the <code>metadata</code> accordingly and you will see a green tick.</p> <p></p> <p>If you are loading a czi, the metadata fields should be populated automatically.</p> <p>To <code>Preview</code> the deskewed image, click <code>Preview</code> and choose the appropriate <code>channel</code> and <code>time</code>.</p> <p>You should see the deskewed image appear as an extra layer with the <code>Preview</code> suffix attached to it.</p> <p></p> Extra_info <p>If you look at the terminal after deskew, you should see the settings used and any other metadata associated with the dataset. It is handy for troubleshooting.</p> <p>From version 1.0.3 onwards, we have an option to show the Deskewed image without actually deskewing it.  It does not create a new image, but simply transforms the image in the canvas to a deskewed image.  This can be useful for quick preview of the data.</p> <p>To do this, once the plugin is initialized, click on <code>Quick Deskew</code>.</p> <p></p> <p>Once you click it, you can view the deskewed image in the napari image canvas.</p> <p></p> <p>You may get the following warning: <code>Non-orthogonal slicing is being requested, but is not fully supported. Data is displayed without applying an out-of-slice rotation or shear component.!</code> This is absolutely fine. It just means the image won't be displayed as deskewed in 2D mode. Hence, why we enable 3D mode.</p> <p>Here is an example of browsing through a timeseries</p> <p></p> <p>The smoothness of this interactivity will depend on the storage read/write speeds and/or network speeds. For example, if the data is stored on the network, it will be slow to browse timepoints. However, if your data is on your SSD locally, the experience will be much better.</p> <p>Deconvolution is primarily enabled by <code>pycudadecon</code>. For this functionality, you will need the point spread function (PSF) for the corresponding channel, either simulated or experimentally derived. You can find examples here.</p> <p>Important</p> <p>Ensure you are using the right PSF file for each channel. The number and order of the PSF files should match the channels in the image.</p> <p>After loading the image and configuring it in the <code>Deskew</code> tab, select the <code>Deconvolution</code> tab. When you click <code>Enable</code>, you should see a green tick appear next to the name.</p> <p></p> <p>Under processing algorithms only <code>cuda_gpu</code> and <code>cpu</code> are supported. <code>opencl_gpu</code> has not been implemented yet. The next step is to select the PSF files. In this example, we will use the <code>RBC_tiny.czi</code> file</p> <p></p> <ul> <li>PSFs: Use the <code>Select files</code> to select multiple PSF files. As the dataset was acquired in the 48 channel, we use the 488.czi PSF file here.</li> <li>Number of iterations: Try 10 if not sure and increase if needed.</li> <li>Background: Background to subtract. <ul> <li>Automatic: median value of last Z slice will be used</li> <li>Second Last: median value of second last Z slice will be used. This is used in case the last Z slice is incomplete if acquisition is prematurely stopped.</li> <li>Custom: Enter a custom value</li> </ul> </li> </ul> <p>Once you are done, click <code>Preview</code> at the bottom, and select timepoint or channel. You should see output from <code>pycudadecon</code> printed to the terminal.  When complete, a deconvolved image will appear as an extra image layer. Below is an example of the deskewed image without (left) and with (right) deconvolution.</p> <p></p> <p>There are two ways to do the cropping:</p> <ul> <li>Define ROIs within napari-lattice plugin</li> <li>Import ROIs generated elsewhere, such as Fiji ROI Manager.</li> </ul> <p>Define ROIs in napari-lattice</p> <ul> <li>Load and configure the image in the <code>Deskew</code> tab and you should see a green tick. </li> <li>Run Preview to get a deskewed volume. We will use this as a reference to draw ROIs for cropping.</li> <li>Go to the <code>Crop</code> tab and tick the <code>Enabled</code> button to activate cropping.</li> </ul> <p></p> <p>The red text at the bottom indicates that atleast one ROI must be specified.</p> <ul> <li>Click on <code>New Crop</code> at the bottom of the plugin to add a <code>Shapes</code> layer on the left to draw ROIs. This Shapes layer will be called <code>Napari Lattice Crop</code>. Click here for more info on using Shapes layers and drawing shapes.</li> <li>Click on the <code>Napari Lattice Crop</code> Shapes layer and the rectangular ROI tool will be selected by default. </li> <li>Draw an ROI around the region you would like to crop. After defining the ROI, it will appear on the right. </li> <li>Similarly, you can draw multiple ROIs. Each ROI will be an entry in the ROIs box. When you select one of them, the error message below will disappear.</li> </ul> <p></p> <ul> <li>Once you have drawn the ROIs, select one of them, and click <code>Preview</code> to visualize the cropped region. The cropped image will appear as a new layer in the image layer list on the left. </li> </ul> <p></p> <ul> <li>The purpose of the Crop tab is to setup the ROIs. Once you've defined all of them, you can save all of them by configuring it in the <code>Output</code> tab.</li> </ul> <p>Import ROIs</p> <p>We have added support to import ROIs from Fiji ROI Manager file. This workflow exists because the Zeiss lattice lightsheet produces a 2D maximum intensity projection at the end of the acquisition. This image can be used to select ROIs of interest in Fiji. Refer to this page for more instructons on how to generate.</p> <p>MORE INSTRUCTIONS TO BE ADDED</p> <p>Instructions about importing workflows to be added</p> <p>Instructions on how to save files after setting up the other tabs</p>"}]}